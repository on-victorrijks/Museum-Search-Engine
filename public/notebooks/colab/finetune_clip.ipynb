{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "#from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "import csv\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import json\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/content/drive/MyDrive/MASTER_THESIS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "once_runned = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM-R Large Vit-L/14\n",
    "# OpenAI model: OpenAI ViT-L/14 \n",
    "# ==> ViT-L/14 or ViT-L/14@336px ?\n",
    "\n",
    "#model_name = \"openai/clip-vit-large-patch14\"\n",
    "#processor = CLIPProcessor.from_pretrained(model_name)\n",
    "#model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "model_name = \"ViT-L/14\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False) # why double load ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET = pd.read_csv(root + \"fabritius_data_filtered_downloaded.csv\")\n",
    "# Remove rows with corrupted images\n",
    "FULL_DATASET = FULL_DATASET[FULL_DATASET[\"recordID\"] != 11546]\n",
    "FULL_DATASET = FULL_DATASET[FULL_DATASET[\"recordID\"] != 5262]\n",
    "FULL_DATASET = FULL_DATASET.sample(frac=1.0).reset_index(drop=True)\n",
    "FULL_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixPath(path):\n",
    "    return path.replace(\".././\", \"../\")\n",
    "\n",
    "def get_image_path_from_recordID(dataset, recordID):\n",
    "    \"\"\"\n",
    "    Given a recordID, return the local path for its image.\n",
    "    \"\"\"\n",
    "    # Locate row in the downloaded DataFrame\n",
    "    paths = FULL_DATASET[\n",
    "        FULL_DATASET[\"recordID\"] == recordID\n",
    "    ][\"low_res_filename\"].values\n",
    "\n",
    "    if len(paths) == 0:\n",
    "        return None\n",
    "\n",
    "    path = paths[0]\n",
    "    # Merge: IMAGES_FOLDER + path[1:]\n",
    "    merged_path = fixPath(root + \"images/\" + path[1:])\n",
    "    return merged_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CAPTIONS = pd.read_csv(root + \"merged_data_training_set.csv\")\n",
    "# rows: recordID,category,focus,caption\n",
    "TRAINING_CAPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_CAPTIONS = pd.read_csv(root + \"merged_data_validation_set.csv\")\n",
    "# rows: recordID,category,focus,caption\n",
    "VALIDATION_CAPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadatas = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_name\": model_name,\n",
    "    \"pretrained\": model_name,\n",
    "    \"dataset\": {\n",
    "        \"name\": \"training_set_captions\",\n",
    "        \"size\": len(TRAINING_CAPTIONS),\n",
    "        \"size_recordID\": len(TRAINING_CAPTIONS[\"recordID\"].unique()),\n",
    "        \"size_per_category\": TRAINING_CAPTIONS.groupby(\"category\").size().to_dict(),\n",
    "        \"size_per_focus\": TRAINING_CAPTIONS.groupby(\"focus\").size().to_dict(),\n",
    "    },\n",
    "    \"hyperparameters\": {\n",
    "        \"batch_size\": 128, #256, # 32 = 8 <==> 128 = 23\n",
    "        \"num_epochs\": 30,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"betas\": (0.9, 0.98),\n",
    "        \"weight_decay\": 0.2,\n",
    "    }\n",
    "}\n",
    "def getIdentifier():\n",
    "    return model_metadatas[\"timestamp\"]\n",
    "\n",
    "# Save model_metadatas\n",
    "with open(root + f\"model_metadatas_{getIdentifier()}.json\", \"w\") as f:\n",
    "    json.dump(model_metadatas, f)\n",
    "model_metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sets\n",
    "class FinetuningDataset(Dataset):\n",
    "    def __init__(self, dataframe, getImageFromRecordID, preprocess=preprocess):\n",
    "        self.dataframe = dataframe\n",
    "        self.getImageFromRecordID = getImageFromRecordID\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        recordID = row['recordID']\n",
    "        caption = row['caption']\n",
    "        path = self.getImageFromRecordID(self.dataframe, recordID)\n",
    "\n",
    "        path = path.replace(\"internet\", \"Internet\")\n",
    "        path = path.replace(\"Mod\", \"mod\")\n",
    "        path = path.replace(\"Old\", \"old\")\n",
    "        path = path.replace(\"Stefaan\", \"stefaan\")\n",
    "        path = path.replace(\"Art-Foto\", \"art-foto\")\n",
    "        image = Image.open(path)#.convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "\n",
    "        return image, caption, recordID\n",
    "\n",
    "def customBatchBuilder(samples):\n",
    "    images, captions, recordIDs = zip(*samples)\n",
    "    #inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = None\n",
    "    #return inputs, images, captions, recordIDs\n",
    "    return images, captions, recordIDs\n",
    "\n",
    "# Training dataset with only content focus\n",
    "DATASET__TRAINING_ONLY_CONTENT_FOCUS = FinetuningDataset(TRAINING_CAPTIONS[TRAINING_CAPTIONS[\"focus\"] == \"content\"], get_image_path_from_recordID)\n",
    "DATASET__TRAINING_ALL_FOCUS          = FinetuningDataset(TRAINING_CAPTIONS, get_image_path_from_recordID)\n",
    "\n",
    "# Make dataloaders\n",
    "DATALOADER__TRAINING_ONLY_CONTENT_FOCUS = DataLoader(\n",
    "    DATASET__TRAINING_ONLY_CONTENT_FOCUS, \n",
    "    batch_size=model_metadatas[\"hyperparameters\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=customBatchBuilder\n",
    ")\n",
    "DATALOADER__TRAINING_ALL_FOCUS = DataLoader(\n",
    "    DATASET__TRAINING_ALL_FOCUS, \n",
    "    batch_size=model_metadatas[\"hyperparameters\"][\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Print the lengths of the datasets and dataloaders\n",
    "print(\"TRAINING_ONLY_CONTENT_FOCUS:\", len(DATASET__TRAINING_ONLY_CONTENT_FOCUS), \" | \", len(DATALOADER__TRAINING_ONLY_CONTENT_FOCUS))\n",
    "print(\"TRAINING_ALL_FOCUS:\", len(DATASET__TRAINING_ALL_FOCUS), \" | \", len(DATALOADER__TRAINING_ALL_FOCUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training_metrics & Validation sets\n",
    "DATASET__VALIDATION_ALL_FOCUS = FinetuningDataset(VALIDATION_CAPTIONS, get_image_path_from_recordID)\n",
    "DATALOADER__VALIDATION_ALL_FOCUS = DataLoader(DATASET__VALIDATION_ALL_FOCUS, batch_size=model_metadatas[\"hyperparameters\"][\"batch_size\"], shuffle=True, num_workers=0)\n",
    "\n",
    "# Since we have multiple captions per image, we want to make a Dataset that allow us to measure the performance of the model on each focus\n",
    "DATASET__TRAINING_PER_FOCUS     = {}\n",
    "DATASET__VALIDATION_PER_FOCUS   = {}\n",
    "\n",
    "for focus in [\"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "    DATASET__TRAINING_PER_FOCUS[focus]      = FinetuningDataset(TRAINING_CAPTIONS[TRAINING_CAPTIONS[\"focus\"] == focus], get_image_path_from_recordID)\n",
    "    DATASET__VALIDATION_PER_FOCUS[focus]    = FinetuningDataset(VALIDATION_CAPTIONS[VALIDATION_CAPTIONS[\"focus\"] == focus], get_image_path_from_recordID)\n",
    "\n",
    "# Make dataloaders\n",
    "DATALOADER__TRAINING_PER_FOCUS = {}\n",
    "DATALOADER__VALIDATION_PER_FOCUS = {}\n",
    "for focus in [\"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "    DATALOADER__TRAINING_PER_FOCUS[focus] = DataLoader(DATASET__TRAINING_PER_FOCUS[focus], batch_size=model_metadatas[\"hyperparameters\"][\"batch_size\"], shuffle=True, num_workers=0 , collate_fn=customBatchBuilder)\n",
    "    DATALOADER__VALIDATION_PER_FOCUS[focus] = DataLoader(DATASET__VALIDATION_PER_FOCUS[focus], batch_size=model_metadatas[\"hyperparameters\"][\"batch_size\"], shuffle=True, num_workers=0, collate_fn=customBatchBuilder)\n",
    "\n",
    "# Print the lengths of the datasets and dataloaders\n",
    "sizes = pd.DataFrame(columns=[\"focus\", \"Dataset (training)\", \"Dataset (validation)\", \"Dataloader (training)\", \"Dataloader (validation)\"])\n",
    "for focus in [\"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "    sizes.loc[len(sizes)] = [focus, len(DATASET__TRAINING_PER_FOCUS[focus]), len(DATASET__VALIDATION_PER_FOCUS[focus]), len(DATALOADER__TRAINING_PER_FOCUS[focus]), len(DATALOADER__VALIDATION_PER_FOCUS[focus])]\n",
    "\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "  model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=model_metadatas[\"hyperparameters\"][\"learning_rate\"], \n",
    "    weight_decay=model_metadatas[\"hyperparameters\"][\"weight_decay\"],\n",
    "    betas=model_metadatas[\"hyperparameters\"][\"betas\"]\n",
    ")\n",
    "loss_image      = nn.CrossEntropyLoss()\n",
    "loss_caption    = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_position(cosine_similarities):\n",
    "    \"\"\"\n",
    "    What is the Average Position?\n",
    "    ==> The Average Position is a metric that evaluates the effectiveness of a recommendation algorithm.\n",
    "    ==> It is the average of the positions of the correct answers.\n",
    "    ==> The position of a query response is the rank of the first correct answer.\n",
    "    ==> The Average Position is a number between 0 and n, where 0 means that the first correct answer is always ranked first.\n",
    "    \"\"\"\n",
    "    average_position = 0\n",
    "    for i in range(len(cosine_similarities)):\n",
    "        sorted_indices = np.argsort(cosine_similarities[i])[::-1]\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1\n",
    "        average_position += rank\n",
    "    average_position /= len(cosine_similarities)\n",
    "    return average_position\n",
    "\n",
    "def get_MRR(cosine_similarities):\n",
    "    \"\"\"\n",
    "    What is MRR (Mean Reciprocal Rank)?\n",
    "    ==> The Mean Reciprocal Rank is a metric that evaluates the effectiveness of a recommendation algorithm.\n",
    "    ==> It is the average of the reciprocal ranks of the top k items.\n",
    "    ==> The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer.\n",
    "    ==> The MRR is a number between 0 and 1, where 1 means that the first correct answer is always ranked first.\n",
    "    \"\"\"\n",
    "    mrr = 0\n",
    "    for i in range(len(cosine_similarities)):\n",
    "        sorted_indices = np.argsort(cosine_similarities[i])[::-1]\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1\n",
    "        mrr += 1 / rank\n",
    "    mrr /= len(cosine_similarities)\n",
    "    return mrr\n",
    "\n",
    "def get_recall_at_k(cosine_similarities, k):\n",
    "    \"\"\"\n",
    "    What is Recall@k?\n",
    "    ==> The Recall@k is a metric that evaluates the effectiveness of a recommendation algorithm.\n",
    "    ==> It is the proportion of the top k items that are relevant.\n",
    "    ==> The Recall@k is a number between 0 and 1, where 1 means that all top k items are relevant.\n",
    "    \"\"\"\n",
    "    recall_at_k = 0\n",
    "    for i in range(len(cosine_similarities)):\n",
    "        sorted_indices = np.argsort(cosine_similarities[i])[::-1]\n",
    "        if i in sorted_indices[:k]:\n",
    "            recall_at_k += 1\n",
    "    recall_at_k /= len(cosine_similarities)\n",
    "    return recall_at_k\n",
    "\n",
    "def get_nDCG_at_k(cosine_similarities, k):\n",
    "    \"\"\"\n",
    "    What is nDCG@k (Discounted cumulative gain)\n",
    "    ==> The nDCG@k is a metric that evaluates the effectiveness of a recommendation algorithm.\n",
    "    ==> It is the normalized discounted cumulative gain at the top k items.\n",
    "    ==> The nDCG@k is a number between 0 and 1, where 1 means that all top k items are relevant and perfectly ranked.\n",
    "    \"\"\"\n",
    "    nDCG_at_k = 0\n",
    "    for i in range(len(cosine_similarities)):\n",
    "        sorted_indices = np.argsort(cosine_similarities[i])[::-1]\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1\n",
    "        nDCG_at_k += 1 / np.log2(rank + 1) if rank <= k else 0\n",
    "    nDCG_at_k /= len(cosine_similarities)\n",
    "    return nDCG_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_on_dataloader(model, dataset, dataloader, device):\n",
    "    \"\"\"\n",
    "    This function measures various metrics on a dataloader:\n",
    "    - Loss\n",
    "    - Average Position\n",
    "    - MRR (Mean Reciprocal Rank)\n",
    "    - Recall@1, Recall@5, Recall@10\n",
    "    - nDCG@1, nDCG@5, nDCG@10\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_tot = 0\n",
    "    positions = []\n",
    "    recalls = {1: 0, 5: 0, 10: 0}\n",
    "    ndcgs = {1: 0, 5: 0, 10: 0}\n",
    "\n",
    "    images_embeddings = torch.tensor([]).to(device)\n",
    "    texts_embeddings = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, recordIDs in tqdm(dataloader):            \n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Compute the embeddings\n",
    "            image_embeddings = model.encode_image(images)\n",
    "            text_embeddings = model.encode_text(captions)\n",
    "\n",
    "            # Normalize\n",
    "            image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "            text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Recover logits\n",
    "            logits_per_image, logits_per_text = model(images, captions)\n",
    "\n",
    "            # Ground truth\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = (loss_image(logits_per_image, ground_truth) + loss_caption(logits_per_text, ground_truth))/2\n",
    "            loss_tot += loss.item()\n",
    "\n",
    "            # Add the embeddings to the list\n",
    "            images_embeddings = torch.cat((images_embeddings, image_embeddings), 0)\n",
    "            texts_embeddings = torch.cat((texts_embeddings, text_embeddings), 0)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss_per_pair = loss_tot/len(dataset) # Average loss per pair\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarities = images_embeddings @ texts_embeddings.T\n",
    "    similarities = similarities.cpu().numpy()\n",
    "\n",
    "    # Compute the positions\n",
    "    average_position = get_average_position(similarities)\n",
    "    mrr = get_MRR(similarities)\n",
    "    recalls[1] = get_recall_at_k(similarities, 1)\n",
    "    recalls[5] = get_recall_at_k(similarities, 5)\n",
    "    recalls[10] = get_recall_at_k(similarities, 10)\n",
    "    ndcgs[1] = get_nDCG_at_k(similarities, 1)\n",
    "    ndcgs[5] = get_nDCG_at_k(similarities, 5)\n",
    "    ndcgs[10] = get_nDCG_at_k(similarities, 10)\n",
    "\n",
    "    return [loss_per_pair, average_position, mrr, recalls[1], recalls[5], recalls[10], ndcgs[1], ndcgs[5], ndcgs[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, epoch, training_df, validation_df, device):\n",
    "    \"\"\"\n",
    "    There are quite a lot of benchmarks to run:\n",
    "    1) Training set\n",
    "        1.1) Training set (all focus)\n",
    "        1.2) Training set (content focus)\n",
    "        1.3) Training set (emotion focus)\n",
    "        1.4) Training set (colors focus)\n",
    "        1.5) Training set (luminosity focus)\n",
    "    2) Validation set\n",
    "        2.1) Validation set (all focus)\n",
    "        2.2) Validation set (content focus)\n",
    "        2.3) Validation set (emotion focus)\n",
    "        2.4) Validation set (colors focus)\n",
    "        2.5) Validation set (luminosity focus)\n",
    "    ==> The \"all\" row is the mean of the other rows\n",
    "    \"\"\"\n",
    "    def addRow(df, epoch, focus, results):\n",
    "        df.loc[len(df)] = [epoch, focus] + results\n",
    "\n",
    "    # Training set\n",
    "    mean_row = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    for focus in [\"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "        measures = benchmark_on_dataloader(model, TRAINING_CAPTIONS[TRAINING_CAPTIONS[\"focus\"] == focus], DATALOADER__TRAINING_PER_FOCUS[focus], device)\n",
    "        addRow(training_df, epoch, focus, measures)\n",
    "        mean_row = [mean_row[i] + measures[i] for i in range(len(mean_row))]\n",
    "    mean_row = [mean_row[i] / 4 for i in range(len(mean_row))]\n",
    "    addRow(training_df, epoch, \"all\", mean_row)\n",
    "\n",
    "    # Validation set\n",
    "    mean_row = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    for focus in [\"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "        measures = benchmark_on_dataloader(model, VALIDATION_CAPTIONS[VALIDATION_CAPTIONS[\"focus\"] == focus], DATALOADER__VALIDATION_PER_FOCUS[focus], device)\n",
    "        addRow(validation_df, epoch, focus, measures)\n",
    "        mean_row = [mean_row[i] + measures[i] for i in range(len(mean_row))]\n",
    "    mean_row = [mean_row[i] / 4 for i in range(len(mean_row))]\n",
    "    addRow(validation_df, epoch, \"all\", mean_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pd_tables():\n",
    "    \"\"\"\n",
    "    There are quite a lot of benchmarks to run:\n",
    "    1) Training set\n",
    "        1.1) Training set (all focus)\n",
    "        1.2) Training set (content focus)\n",
    "        1.3) Training set (emotion focus)\n",
    "        1.4) Training set (colors focus)\n",
    "        1.5) Training set (luminosity focus)\n",
    "    2) Validation set\n",
    "        2.1) Validation set (all focus)\n",
    "        2.2) Validation set (content focus)\n",
    "        2.3) Validation set (emotion focus)\n",
    "        2.4) Validation set (colors focus)\n",
    "        2.5) Validation set (luminosity focus)\n",
    "    ==> Create two DataFrames, one for training and one for validation\n",
    "    \"\"\"\n",
    "    training_df = pd.DataFrame(columns=[\"epoch\", \"focus\", \"loss\", \"average_position\", \"mrr\", \"recall@1\", \"recall@5\", \"recall@10\", \"ndcg@1\", \"ndcg@5\", \"ndcg@10\"])\n",
    "    validation_df = pd.DataFrame(columns=[\"epoch\", \"focus\", \"loss\", \"average_position\", \"mrr\", \"recall@1\", \"recall@5\", \"recall@10\", \"ndcg@1\", \"ndcg@5\", \"ndcg@10\"])\n",
    "    return training_df, validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mrr for the training and validation sets for each focus\n",
    "def plot_mrr(df, title, save_name):\n",
    "    epochs = sorted(df[\"epoch\"].unique())\n",
    "    mrrs = {focus: [] for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]}\n",
    "\n",
    "    for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "        for epoch in epochs:\n",
    "            mrr = df[(df[\"epoch\"] == epoch) & (df[\"focus\"] == focus)][\"mrr\"].values[0]\n",
    "            mrrs[focus].append(mrr)\n",
    "\n",
    "    if len(epochs)==1:\n",
    "      # Bar plot\n",
    "      plt.figure(figsize=(10, 5))\n",
    "\n",
    "      # Sorted by MRR (high to low)\n",
    "      mrrs = {k: v for k, v in sorted(mrrs.items(), key=lambda item: item[1][-1], reverse=True)}\n",
    "\n",
    "      for focus, mrr in mrrs.items():\n",
    "        plt.bar(focus, mrr[-1], label=focus)\n",
    "\n",
    "      plt.title(title)\n",
    "      plt.xlabel(\"Focus\")\n",
    "      plt.ylabel(\"MRR\")\n",
    "      plt.ylim(0, 1)\n",
    "      plt.grid()\n",
    "      plt.savefig(root + save_name + \".pdf\")\n",
    "      plt.legend()\n",
    "    else:\n",
    "      # Plot the MRR for each focus (different colors) per epoch\n",
    "      plt.figure(figsize=(10, 5))\n",
    "\n",
    "      for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "          plt.plot(epochs, mrrs[focus], label=focus)\n",
    "\n",
    "      plt.title(title)\n",
    "      plt.xlabel(\"Epoch\")\n",
    "      plt.ylabel(\"MRR\")\n",
    "      plt.legend()\n",
    "      plt.ylim(0, 1)\n",
    "      plt.grid()\n",
    "      plt.savefig(root + save_name + \"_\" + getIdentifier() + \".pdf\")\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(training_df, validation_df, title, save_name):\n",
    "    epochs = sorted(training_df[\"epoch\"].unique())\n",
    "    losses = {focus: [] for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]}\n",
    "\n",
    "    for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "        for epoch in epochs:\n",
    "            loss = training_df[(training_df[\"epoch\"] == epoch) & (training_df[\"focus\"] == focus)][\"loss\"].values[0]\n",
    "            losses[focus].append(loss)\n",
    "\n",
    "    # Plot the loss for each focus (different colors) per epoch\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "        plt.plot(epochs, losses[focus], label=focus)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(root + save_name + \"_\" + getIdentifier() + \".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "once_runned = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics on the untrained model\n",
    "if not once_runned:\n",
    "    # Compute the metrics on the untrained model (SHOULD NOT BE RUN AGAIN)\n",
    "    untrained_training_df, untrained_validation_df = create_pd_tables()\n",
    "    run_benchmark(model, 0, untrained_training_df, untrained_validation_df, device)\n",
    "    plot_mrr(untrained_training_df, \"MRR on the training set (untrained model)\", \"untrained_training_mrr\")\n",
    "    plot_mrr(untrained_validation_df, \"MRR on the validation set (untrained model)\", \"untrained_validation_mrr\")\n",
    "    once_runned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(epoch, training_df, validation_df):\n",
    "    combined_df = pd.DataFrame(columns=[\"Metric name\", \"Training\", \"Validation\"])\n",
    "    for metric in [\"loss\", \"average_position\", \"mrr\", \"recall@1\", \"ndcg@1\"]:\n",
    "      for focus in [\"all\", \"content\", \"emotion\", \"colors\", \"luminosity\"]:\n",
    "            training_metric = training_df[(training_df[\"epoch\"] == epoch) & (training_df[\"focus\"] == focus)][metric].values[0]\n",
    "            validation_metric = validation_df[(validation_df[\"epoch\"] == epoch) & (validation_df[\"focus\"] == focus)][metric].values[0]\n",
    "            combined_df.loc[len(combined_df)] = [metric + \" (\" + focus + \")\", training_metric, validation_metric]\n",
    "\n",
    "    for metric in [\"loss\", \"average_position\", \"mrr\", \"recall@1\", \"ndcg@1\"]:\n",
    "      sub_df = combined_df[combined_df[\"Metric name\"].str.contains(metric)]\n",
    "      # Sort\n",
    "      sub_df = sub_df.sort_values(by=\"Training\", ascending=True)\n",
    "      print(f\"Metric: {metric}\")\n",
    "      print(sub_df)\n",
    "      print()\n",
    "\n",
    "# Test\n",
    "printMetrics(0, untrained_training_df, untrained_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_one_epoch(model, epoch, training_df, validation_df, dataloader, device):\n",
    "    model.train()\n",
    "\n",
    "    for images, captions, recordIDs in tqdm(dataloader):            \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Recover logits\n",
    "        logits_per_image, logits_per_text = model(images, captions)\n",
    "\n",
    "        # Ground truth\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = (loss_image(logits_per_image, ground_truth) + loss_caption(logits_per_text, ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item()}\")\n",
    "\n",
    "    # Compute the metrics for this epoch\n",
    "    print(\"Running benchmark...\")\n",
    "    run_benchmark(model, epoch, training_df, validation_df, device)\n",
    "\n",
    "    # Print the metrics\n",
    "    printMetrics(epoch, training_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, type, epoch):\n",
    "    fullIdentifier = model_metadatas[\"timestamp\"] + \"_\" + type + \"_\" + str(epoch)\n",
    "    model.save_pretrained(root + \"models/\" + fullIdentifier + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop (only content focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial model weights\n",
    "#model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)\n",
    "print(\"Model reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_onlyFocus, validation_df_onlyFocus = create_pd_tables()\n",
    "# Copy the rows from the untrained model\n",
    "for i, row in untrained_training_df.iterrows():\n",
    "    training_df_onlyFocus.loc[len(training_df_onlyFocus)] = row\n",
    "for i, row in untrained_validation_df.iterrows():\n",
    "    validation_df_onlyFocus.loc[len(validation_df_onlyFocus)] = row\n",
    "\n",
    "print(len(untrained_training_df), len(training_df_onlyFocus))\n",
    "print(len(untrained_validation_df), len(validation_df_onlyFocus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (only content focus)\n",
    "for epoch in range(1, model_metadatas[\"hyperparameters\"][\"num_epochs\"]+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_for_one_epoch(model, epoch, training_df_onlyFocus, validation_df_onlyFocus, DATALOADER__TRAINING_ONLY_CONTENT_FOCUS, device)\n",
    "save_model_weights(model, \"onlyFocus\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses for the training and validation sets for each focus\n",
    "plot_losses(training_df_onlyFocus, validation_df_onlyFocus, \"Losses on the training and validation sets (only content focus)\", \"losses_only_content_focus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MRR for the training and validation sets for each focus\n",
    "plot_mrr(training_df_onlyFocus, \"MRR on the training set (only content focus)\", \"training_mrr_onlyFocus\")\n",
    "plot_mrr(validation_df_onlyFocus, \"MRR on the validation set (only content focus)\", \"validation_mrr_onlyFocus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop (all focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial model weights\n",
    "#model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)\n",
    "print(\"Model reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_allFocus, validation_df_allFocus = create_pd_tables()\n",
    "# Copy the rows from the untrained model\n",
    "for i, row in untrained_training_df.iterrows():\n",
    "    training_df_allFocus.loc[len(training_df_allFocus)] = row\n",
    "for i, row in untrained_validation_df.iterrows():\n",
    "    validation_df_allFocus.loc[len(validation_df_allFocus)] = row\n",
    "\n",
    "print(len(untrained_training_df), len(training_df_allFocus))\n",
    "print(len(untrained_validation_df), len(validation_df_allFocus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (all focus)\n",
    "for epoch in range(1, model_metadatas[\"hyperparameters\"][\"num_epochs\"]+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_for_one_epoch(model, epoch, training_df_allFocus, validation_df_allFocus, DATALOADER__TRAINING_ALL_FOCUS, device)\n",
    "save_model_weights(model, \"allFocus\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses for the training and validation sets for each focus\n",
    "plot_losses(training_df_allFocus, validation_df_allFocus, \"Losses on the training and validation sets (all focus)\", \"losses_all_focus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MRR for the training and validation sets for each focus\n",
    "plot_mrr(training_df_allFocus, \"MRR on the training set (all focus)\", \"training_mrr_allFocus\")\n",
    "plot_mrr(validation_df_allFocus, \"MRR on the validation set (all focus)\", \"validation_mrr_allFocus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
